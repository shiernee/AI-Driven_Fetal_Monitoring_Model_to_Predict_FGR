{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoojAd4FNKOB"
      },
      "source": [
        "# 1.Import and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np2kGHS0NKOE"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# import tabpfn\n",
        "\n",
        "# from autosklearn.classification import AutoSklearnClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, PowerTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score, classification_report, roc_curve, confusion_matrix, plot_roc_curve, precision_score, recall_score\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTENC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import xgboost\n",
        "import lightgbm\n",
        "import catboost\n",
        "\n",
        "pd.set_option('display.max_rows', 30)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def convert_feature_to_label(df, col):\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    return df\n",
        "\n",
        "def convert_feature_to_one_hot(df, col):\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    oh = OneHotEncoder(handle_unknown='ignore')\n",
        "    df_oh = pd.DataFrame(oh.fit_transform(df[[col]]).toarray())\n",
        "    df = df.join(df_oh)\n",
        "\n",
        "    rename_dict = {}\n",
        "\n",
        "    for i, name in zip(df_oh.columns, le.classes_):\n",
        "        rename_dict[i] = str(col) + '_' + str(name)\n",
        "\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    df.drop(col, axis = 1, inplace = True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def scale_feature(col, method='std'):\n",
        "    if method == 'std':\n",
        "        std = np.std(col)\n",
        "        mean = np.mean(col)\n",
        "        return (col - mean) / std\n",
        "    elif method =='minmax':\n",
        "        min = np.min(col)\n",
        "        max = np.max(col)\n",
        "        return (col - min) / (max - min)\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred, metrics=[]):\n",
        "    out = []\n",
        "    #y_pred = y_pred >= 0.8357\n",
        "    for met in metrics:\n",
        "        if not met == roc_auc_score:\n",
        "            out.append(met(y_true, y_pred.argmax(1)))\n",
        "        else:\n",
        "            out.append(met(y_true, np.max(y_pred, 1)))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def calc_class_weights(df, target='sga', type='log'):\n",
        "    if type == 'log':\n",
        "        valid_class = df[target].value_counts().sort_index().values\n",
        "        # 1 / (1+ln(x))\n",
        "        class_weights = 1./np.log1p(valid_class)\n",
        "        # Normalize class_weights and multiply with number of classes\n",
        "        class_weights = class_weights / class_weights.sum() * 2\n",
        "\n",
        "    elif type == 'normal':\n",
        "        classes = df[target].value_counts().sort_index().values\n",
        "        class_weights = classes/df.shape[0]\n",
        "        class_weights = 1 - class_weights\n",
        "        class_weights = class_weights / class_weights.sum() * 2\n",
        "\n",
        "    else:\n",
        "        class_weights = None\n",
        "\n",
        "    return class_weights\n",
        "\n",
        "# def ohe(df):\n",
        "#     # OHE for categorical data\n",
        "#     if set(('presentation', 'placenta_site')).issubset(df.columns):\n",
        "#         for col in ['presentation', 'placenta_site']:\n",
        "#             df = convert_feature_to_one_hot(df, col)\n",
        "#     for col in ['cord', 'hypertension', 'diabetes']:\n",
        "#         df = convert_feature_to_one_hot(df, col)\n",
        "#     for col in ['gender', 'smoking']:\n",
        "#         df = convert_feature_to_label(df, col)\n",
        "\n",
        "#     # OHE for ordinal data\n",
        "#     df = df.replace({'oligohydramnios' : 0, 'normal' : 1, 'polyhydramnios' : 2})\n",
        "#     return df\n",
        "def ohe(df):\n",
        "    # OHE for categorical data\n",
        "    categorical_cols = ['presentation', 'placenta_site', 'cord', 'hypertension', 'diabetes', 'gender', 'smoking']\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df = convert_feature_to_one_hot(df, col)\n",
        "\n",
        "    # OHE for ordinal data: 'af' categories\n",
        "    df = df.replace({'oligohydramnios' : 0, 'normal' : 1, 'polyhydramnios' : 2, 'increased': 3, 'reduced' : 4, 'anhydramnios' : 5})\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def data_impute(df):\n",
        "    # Iterative data imputation\n",
        "    imputer = IterativeImputer(random_state = 123)\n",
        "    imputed = imputer.fit_transform(df)\n",
        "    return pd.DataFrame(imputed, columns = df.columns)\n",
        "\n",
        "\n",
        "def remove_multicollinearity(df, thresh = 10):\n",
        "    vif_info = pd.DataFrame()\n",
        "    vif_info['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "    vif_info['Column'] = df.columns\n",
        "    lst = vif_info.values.tolist()\n",
        "    num_lst = [row[1] for row in lst if row[0] != np.inf]\n",
        "    num_lst = df[num_lst]\n",
        "\n",
        "    idx = 100\n",
        "    while idx >= thresh:\n",
        "        vif_info = pd.DataFrame()\n",
        "        vif_info['VIF'] = [variance_inflation_factor(num_lst.values, i) for i in range(num_lst.shape[1])]\n",
        "        vif_info['Column'] = num_lst.columns\n",
        "        lst =  vif_info.values.tolist()\n",
        "        idx = [index for index, item in enumerate(lst) if item == max(lst)][0]\n",
        "        if idx >= thresh:\n",
        "            df.drop(lst[idx[0]], axis = 1, inplace = True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def ignore_low_variance(df, thresh = 0.1, label = 'sga'):\n",
        "    label_col = df[label]\n",
        "    selector = VarianceThreshold(threshold = thresh)\n",
        "    temp = pd.DataFrame(selector.fit_transform(df))\n",
        "    df = df.loc[:, selector.get_support()]\n",
        "    df[label] = label_col\n",
        "    return df\n",
        "\n",
        "\n",
        "def pca(df, num_comp):\n",
        "    # Normalization and transformation for numerical data\n",
        "    continuous_col = ['ac', 'bpd', 'cm', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'hl', 'tcd', 'mother_age_at_start_date', 'mother_height', 'mother_weight']\n",
        "    # if 'mother_height' in df.columns:\n",
        "    #     continuous_col = ['ac', 'bpd', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'mother_age_at_start_date', 'mother_height', 'mother_weight']\n",
        "    # else:\n",
        "    #     continuous_col = ['ac', 'bpd', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'mother_age_at_start_date', 'bmi']\n",
        "\n",
        "    # print(continuous_col)\n",
        "\n",
        "    for col in continuous_col:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                df[col] = scale_feature(df[col], method='std')\n",
        "                df[col] = PowerTransformer(method=\"yeo-johnson\", standardize=False, copy=True).fit_transform(df[col])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    df_cont = df[continuous_col]\n",
        "    df_cat_label = df.drop(df_cont, axis = 1)\n",
        "    pca = PCA(n_components = num_comp)\n",
        "    temp = pd.DataFrame(pca.fit_transform(df_cont))\n",
        "    temp = temp.rename(columns={0: 'ac', 1: 'bpd', 2: 'cm', 3: 'efw_centile', 4: 'efw', 5: 'fl', 6: 'ga', 7: 'hc', 8: 'hl', 9: 'tcd', 10: 'mother_age_at_start_date', 11: 'mother_height', 12: 'mother_weight'})\n",
        "\n",
        "    # if 'mother_height' in df.columns:\n",
        "    #     temp = temp.rename(columns={0: 'ac', 1: 'bpd', 2: 'cm', 3: 'efw_centile', 4: 'efw', 5: 'fl', 6: 'ga', 7: 'hc', 8: 'hl', 9: 'tcd', 10: 'mother_age_at_start_date', 11: 'mother_height', 12: 'mother_weight'})\n",
        "    # else:\n",
        "    #     temp = temp.rename(columns={0: 'ac', 1: 'bpd', 2: 'cm', 3: 'efw_centile', 4: 'efw', 5: 'fl', 6: 'ga', 7: 'hc', 8: 'hl', 9: 'tcd', 10: 'mother_age_at_start_date', 11: 'bmi'})\n",
        "    # print(temp.columns)\n",
        "    # print(pca.explained_variance_ratio_)\n",
        "    result = pd.concat([df_cat_label, temp], axis = 1)\n",
        "    return result\n",
        "\n",
        "def pca_bmi(df, num_comp):\n",
        "    # Normalization and transformation for numerical data\n",
        "    continuous_col = ['ac', 'bpd', 'cm', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'hl', 'tcd', 'mother_age_at_start_date', 'bmi'] #\n",
        "\n",
        "    for col in continuous_col:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                df[col] = scale_feature(df[col], method='std')\n",
        "                df[col] = PowerTransformer(method=\"yeo-johnson\", standardize=False, copy=True).fit_transform(df[col])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    df_cont = df[continuous_col]\n",
        "    df_cat_label = df.drop(df_cont, axis = 1)\n",
        "    pca = PCA(n_components = num_comp)\n",
        "    temp = pd.DataFrame(pca.fit_transform(df_cont))\n",
        "    temp = temp.rename(columns={0: 'ac', 1: 'bpd', 2: 'cm', 3: 'efw_centile', 4: 'efw', 5: 'fl', 6: 'ga', 7: 'hc', 8: 'hl', 9: 'tcd', 10: 'mother_age_at_start_date', 11: 'bmi'}) #\n",
        "\n",
        "    # print(pca.explained_variance_ratio_)\n",
        "    result = pd.concat([df_cat_label, temp], axis = 1)\n",
        "    return result\n",
        "\n",
        "## method 1\n",
        "def ToNormal(df, type = 'MinMax'):\n",
        "    # MINMAX normalization for numerical data\n",
        "    if 'mother_height' in df.columns:\n",
        "        continuous_col = ['ac', 'bpd', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'mother_age_at_start_date', 'mother_height', 'mother_weight']\n",
        "    else:\n",
        "        continuous_col = ['ac', 'bpd', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'mother_age_at_start_date', 'bmi']\n",
        "\n",
        "    if type == 'MinMax':\n",
        "        scaler = MinMaxScaler()\n",
        "    if type == 'ZScore':\n",
        "    ## z-score\n",
        "        scaler = StandardScaler()\n",
        "    df[continuous_col] = scaler.fit_transform(df[continuous_col])\n",
        "    df_cont = df[continuous_col]\n",
        "    df_cat_label = df.drop(df_cont, axis=1)\n",
        "    return df_cont.join(df_cat_label)\n",
        "\n",
        "## method 2\n",
        "# def ToNormal(df):\n",
        "    # MINMAX normalization for numerical data\n",
        "\n",
        "    # continuous_col = ['ac', 'bpd', 'efw_centile', 'efw', 'fl', 'ga', 'hc', 'mother_age_at_start_date', 'mother_height', 'mother_weight']\n",
        "    # for col in continuous_col:\n",
        "    #     try:\n",
        "    #         df[col] = scale_feature(df[col], method='minmax')\n",
        "    #         # df[col] = PowerTransformer(method=\"yeo-johnson\", standardize=False, copy=True).fit_transform(df[col])\n",
        "    #     except:\n",
        "    #         continue\n",
        "    # df_cat_label = df.drop(continuous_col, axis = 1)\n",
        "    # return df_cat_label.join(df[continuous_col])\n",
        "\n",
        "\n",
        "def sgkf(df, label = 'sga'):\n",
        "    sgkf = StratifiedGroupKFold(shuffle=True, random_state=123, n_splits = 5)\n",
        "\n",
        "    df['fold'] = -1\n",
        "\n",
        "    for fold_num, (_, val_idx) in enumerate(sgkf.split(df, df[label], groups=df.id)):\n",
        "        df.loc[val_idx, 'fold'] = fold_num\n",
        "\n",
        "    # tri2[tri2['fold'] == 0].sga.value_counts()\n",
        "    return df.drop('id', axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR3aRF_SNKOJ"
      },
      "source": [
        "# 2.Model\n",
        "- LR\n",
        "- LightLGM\n",
        "- SVCrbf\n",
        "- SVClr\n",
        "- Catboost\n",
        "- Gaussian\n",
        "- lgbm\n",
        "- xgboost\n",
        "- random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNIB3Wx3NKOK"
      },
      "source": [
        "## LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpqtmnahNKOK"
      },
      "outputs": [],
      "source": [
        "def LR(df, ctrl, label = 'sga'):\n",
        "\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "    # print(train_cols)\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'class_weight' : ['balanced']\n",
        "            }\n",
        "\n",
        "        lr = LogisticRegression(class_weight='balanced')\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        # sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        # lr.fit(train_X, train_Y)\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "        grid_search.fit(train_X, train_Y)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        lr = LogisticRegression(**best_params)\n",
        "        lr.fit(train_X, train_Y)\n",
        "\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "\n",
        "    # print(train_cols)\n",
        "\n",
        "    print()\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "\n",
        "    return\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgIcAWqjNKOU"
      },
      "source": [
        "## LightLGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jMorjbSNKOU"
      },
      "outputs": [],
      "source": [
        "def LightLGM(df, ctrl, label = 'sga'):\n",
        "# lr = LogisticRegression()\n",
        "# lr = lightgbm.LGBMClassifier('gbdt', learning_rate=0.2)\n",
        "# lr = xgboost.XGBClassifier()\n",
        "# lr= tabpfn.TabPFNClassifier(N_ensemble_configurations=16, only_inference=False, combine_preprocessing=True)\n",
        "# lr = RandomForestClassifier(random_state=0, criterion='gini', n_estimators=300, max_depth=100)\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "    reg_range = np.arange(0.1, 1.1, 0.1)\n",
        "    rate = np.arange(0, 1, 0.01)\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'boosting_type': ['gbdt'], #default\n",
        "            'class_weight': ['balanced'], #\n",
        "            # 'colsample_bytree': [1.0], #default\n",
        "            # 'importance_type': ['split'], #default\n",
        "            'learning_rate': [0.4], #\n",
        "            'max_depth': [-1], #\n",
        "            'min_child_samples': [13], #\n",
        "            'min_child_weight': [0.001], #default\n",
        "            'min_split_gain': [0.0], #\n",
        "            'n_estimators': [100], #\n",
        "            'n_jobs': [-1], #default\n",
        "            'num_leaves': [27], #\n",
        "            'random_state': [123], #\n",
        "            'reg_alpha': [0.0], #\n",
        "            'reg_lambda': [0.0],\n",
        "            'silent': [True],\n",
        "            'subsample': [1.], #\n",
        "            # 'subsample_for_bin': [200000], #default\n",
        "            # 'subsample_freq': [0], #default\n",
        "              }\n",
        "\n",
        "        lr = lightgbm.LGBMClassifier()\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        lr.fit(train_X, train_Y)\n",
        "\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5)\n",
        "        grid_search.fit(train_X, train_Y, sample_weight=sample_weights)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "\n",
        "        lr = lightgbm.LGBMClassifier(**best_params) #\n",
        "        lr = lightgbm.LGBMClassifier(class_weight='balanced', learning_rate=0.4, min_child_samples=13,\n",
        "               num_leaves=27, random_state=123)\n",
        "        lr.fit(train_X, train_Y, sample_weight=sample_weights, verbose=100)\n",
        "\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "\n",
        "    print(lr, lr.get_params())\n",
        "    # print(df.columns)\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "    # lst.to_csv('../data/tri3_predict_proba.csv', index_label=None, index=None)\n",
        "    # return np.mean(oof_roc_auc)\n",
        "    return\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4F4Apl9NKOV"
      },
      "source": [
        "## SVC rbf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyz_2-ExNKOV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def SVCrbf(df, ctrl, label = 'sga'):\n",
        "\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    # class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "    degrees = np.arange(1, 11, 3)\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'kernel': ['rbf'],\n",
        "            'C' : [1],  # 76.48, 77.5\n",
        "            'gamma' : ['scale'],\n",
        "            'probability' : [True],\n",
        "            'random_state': [123],\n",
        "            'degree' : [1] #degrees\n",
        "            }\n",
        "\n",
        "        lr = SVC(probability = True, kernel='rbf', random_state = 123) #, gamma='scale' C = 0.1, degree = 8,kernel = 'rbf' / 'poly' / 'linear', gamma = 'auto',\n",
        "\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        # sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        # lr.fit(train_X, train_Y)\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "        grid_search.fit(train_X, train_Y)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        lr = SVC(**best_params)\n",
        "        lr.fit(train_X, train_Y)\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mv89zENKOW"
      },
      "source": [
        "## SVC lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ_NdXUtNKOW"
      },
      "outputs": [],
      "source": [
        "def SVClr(df, ctrl, label = 'sga'):\n",
        "# lr = LogisticRegression()\n",
        "# lr = lightgbm.LGBMClassifier('gbdt', learning_rate=0.2)\n",
        "# lr = xgboost.XGBClassifier()\n",
        "# lr= tabpfn.TabPFNClassifier(N_ensemble_configurations=16, only_inference=False, combine_preprocessing=True)\n",
        "# lr = RandomForestClassifier(random_state=0, criterion='gini', n_estimators=300, max_depth=100)\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    # class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'kernel': ['linear'],\n",
        "            'C' : [1],  # 76.48, 77.5\n",
        "            'gamma' : ['scale'],\n",
        "            'probability' : [True],\n",
        "            'random_state': [123],\n",
        "            'degree' : [3]\n",
        "            }\n",
        "\n",
        "        lr = SVC(probability = True, kernel='linear', random_state = 123, C = 1) #, gamma='scale' C = 0.1, degree = 8,kernel = 'rbf' / 'poly' / 'linear', gamma = 'auto',\n",
        "\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        # sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        # lr.fit(train_X, train_Y)\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "        grid_search.fit(train_X, train_Y)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        lr = SVC(**best_params)\n",
        "        lr.fit(train_X, train_Y)\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZvp0AotNKOW"
      },
      "source": [
        "## Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbOXHXZSNKOX"
      },
      "outputs": [],
      "source": [
        "def Catboost(df, ctrl, label='sga'):\n",
        "\n",
        "    train_cols = df.columns\n",
        "    train_cols = train_cols.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "    oof_thresh = []\n",
        "    temp = None\n",
        "\n",
        "    class_weights = calc_class_weights(df, target=label, type='normal')\n",
        "\n",
        "    ran = np.arange(0.01,0.07, 0.01)\n",
        "    ran1 = np.arange(0.1,0.7,0.1)\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "\n",
        "            'iterations': [300], ## number↑, time cost↑； ROC↓, Accuracy↑\n",
        "            'learning_rate': [0.01], ## √ rate↑, ROC↓, Accuracy↓\n",
        "            'depth': [6],\n",
        "            'l2_leaf_reg': [1], # √ [1, 2, 3]\n",
        "            'random_strength': [0.2], # √ 0.3, 0.1, 0.2,\n",
        "            # 'bagging_temperature': [None],\n",
        "            'border_count': [32], # √ 64, 128， time↑  auc↓ roc↑\n",
        "            'subsample': [0.1],# √, (0.2, 0.6)[0.1] auc↑ roc↓\n",
        "\n",
        "            'bootstrap_type': ['Bernoulli'], # √ CPU - 'MVS'; GPU-'Bayesian'， 'Poisson'\n",
        "            'boosting_type': ['Ordered'], # √ 'Plain'\n",
        "            'feature_border_type': ['GreedyLogSum'], # √, 'Median', 'Uniform', 'UniformAndQuantiles', 'GreedyLogSum', 'MaxLogSum', 'MinEntropy'\n",
        "            'fold_permutation_block': [1], # √ (1,+inf)\n",
        "            'grow_policy': ['SymmetricTree'], # √ 'SymmetricTree','Lossguide','Depthwise'\n",
        "            'leaf_estimation_backtracking': ['AnyImprovement'], #√\n",
        "            'leaf_estimation_iterations': [1],#√\n",
        "            'leaf_estimation_method': ['Newton'], # √ 'Newton', 'Gradient'\n",
        "            'nan_mode': ['Min'],# √ 'Min', 'Max', 'Forbidden'\n",
        "            'sampling_frequency': ['PerTreeLevel'],# √ 'PerTreeLevel', 'PerTree'\n",
        "            'use_best_model': [True], # √\n",
        "\n",
        "            'custom_metric' : ['AUC', 'BalancedAccuracy', 'F1'], # √\n",
        "            # 'custom_metric' : ['BalancedAccuracy', 'AUC', 'F1'], # √\n",
        "\n",
        "            # 'scale_pos_weight': [ class_weights[1]/class_weights[0]], #1,\n",
        "            # 'loss_function' : ['Logloss'], #,'CrossEntropy', 'F1', 'AUC' √\n",
        "            'loss_function' : ['CrossEntropy'] # if 'scale_pos_weight' default √\n",
        "            }\n",
        "        cat_model = CatBoostClassifier()\n",
        "\n",
        "        ros = RandomOverSampler(random_state=123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "\n",
        "        grid_search = GridSearchCV(cat_model, param_grid=param_grid, cv=5)\n",
        "        grid_search.fit(train_X, train_Y, eval_set=(train_X, train_Y), sample_weight=sample_weights, use_best_model=True, verbose=0)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        lr = CatBoostClassifier(**best_params)\n",
        "        lr.fit(train_X, train_Y, eval_set=(train_X, train_Y), sample_weight=sample_weights, verbose=0)\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(\n",
        "            val_df[label],\n",
        "            out,\n",
        "            metrics=[\n",
        "                balanced_accuracy_score,\n",
        "                roc_auc_score,\n",
        "                f1_score,\n",
        "                precision_score,\n",
        "                recall_score\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        threshold = []\n",
        "        accuracy = []\n",
        "        for p in np.unique(lr.predict_proba(train_X)[:,1]):\n",
        "            threshold.append(p)\n",
        "            y_pred = (lr.predict_proba(train_X)[:,1] >= p).astype(int)\n",
        "            accuracy.append(balanced_accuracy_score(train_Y, y_pred))\n",
        "        thresh = threshold[np.argmax(accuracy)]\n",
        "\n",
        "        # lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "        oof_thresh.append(thresh)\n",
        "        val_df = pd.concat([val_df, pd.DataFrame(out2)], axis = 1)\n",
        "        temp = pd.concat([temp, val_df], axis = 0)\n",
        "\n",
        "    # print(classification_report(val_df.sga, out2, target_names=['non-sga', 'sga']))\n",
        "    print(best_params)\n",
        "    print(f'OOF Threshold: {np.mean(oof_thresh):.4f} (±{np.std(oof_thresh):.4f})')\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPhMAwDFNKOX"
      },
      "source": [
        "## Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2lXAxRINKOX"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
        "def Gaussian(df, ctrl, label = 'sga'):\n",
        "# lr = LogisticRegression()\n",
        "# lr = lightgbm.LGBMClassifier('gbdt', learning_rate=0.2)\n",
        "# lr = xgboost.XGBClassifier()\n",
        "# lr= tabpfn.TabPFNClassifier(N_ensemble_configurations=16, only_inference=False, combine_preprocessing=True)\n",
        "# lr = RandomForestClassifier(random_state=0, criterion='gini', n_estimators=300, max_depth=100)\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "    reg_range = np.arange(0.1, 1.1, 0.1)\n",
        "    rate = np.arange(0, 1, 0.01)\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'kernel': [RBF()],\n",
        "            # 'alpha': [0.01, 0.1, 1.0],\n",
        "            'optimizer': ['fmin_l_bfgs_b'],\n",
        "            'n_restarts_optimizer': [1],\n",
        "            'random_state': [123]\n",
        "            }\n",
        "        lr = GaussianProcessClassifier()\n",
        "\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        # lr.fit(train_X, train_Y)\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "        grid_search.fit(train_X, train_Y)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "        lr = GaussianProcessClassifier(**best_params)\n",
        "        lr.fit(train_X, train_Y)\n",
        "\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "    print(train_cols)\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38--jc7fNKOY"
      },
      "source": [
        "## lgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1rb74cBNKOY"
      },
      "outputs": [],
      "source": [
        "def lgbm(df, ctrl, label = 'sga'):\n",
        "# lr = LogisticRegression()\n",
        "# lr = lightgbm.LGBMClassifier('gbdt', learning_rate=0.2)\n",
        "# lr = xgboost.XGBClassifier()\n",
        "# lr= tabpfn.TabPFNClassifier(N_ensemble_configurations=16, only_inference=False, combine_preprocessing=True)\n",
        "# lr = RandomForestClassifier(random_state=0, criterion='gini', n_estimators=300, max_depth=100)\n",
        "    lst = pd.DataFrame()\n",
        "    train_cols = df.columns.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "    reg_range = np.arange(0.1, 1.1, 0.1)\n",
        "    rate = np.arange(0, 1, 0.01)\n",
        "\n",
        "    for fold in range(5):\n",
        "        param_grid = {\n",
        "            'boosting_type': ['gbdt'], #default\n",
        "            'class_weight': ['balanced'], #\n",
        "            # 'colsample_bytree': [1.0], #default\n",
        "            # 'importance_type': ['split'], #default\n",
        "            'learning_rate': [0.4], #\n",
        "            'max_depth': [-1], #\n",
        "            'min_child_samples': [13], #\n",
        "            'min_child_weight': [0.001], #default\n",
        "            'min_split_gain': [0.0], #\n",
        "            'n_estimators': [100], #\n",
        "            'n_jobs': [-1], #default\n",
        "            'num_leaves': [27], #\n",
        "            'random_state': [123], #\n",
        "            'reg_alpha': [0.0], #\n",
        "            'reg_lambda': [0.0],\n",
        "            'silent': [True],\n",
        "            'subsample': [1.], #\n",
        "            # 'subsample_for_bin': [200000], #default\n",
        "            # 'subsample_freq': [0], #default\n",
        "              }\n",
        "\n",
        "        lr = lightgbm.LGBMClassifier()\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "        # lr.fit(train_X, train_Y)\n",
        "\n",
        "\n",
        "        grid_search = GridSearchCV(lr, param_grid=param_grid, cv=5)\n",
        "        grid_search.fit(train_X, train_Y, sample_weight=sample_weights)\n",
        "        best_params = grid_search.best_params_\n",
        "\n",
        "\n",
        "        lr = lightgbm.LGBMClassifier(**best_params)\n",
        "        lr.fit(train_X, train_Y, sample_weight=sample_weights, verbose=100)\n",
        "\n",
        "\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "        lst = pd.concat([lst, pd.concat([val_df, pd.DataFrame(out), pd.DataFrame(out2)], axis = 1)], axis = 0)\n",
        "    print(train_cols)\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "    # lst.to_csv('../data/tri3_predict_proba.csv', index_label=None, index=None)\n",
        "    # return np.mean(oof_roc_auc)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwiJZpMyNKOY"
      },
      "source": [
        "## xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT64qWTwNKOZ"
      },
      "outputs": [],
      "source": [
        "def xgboost(df, ctrl, label = 'sga'):\n",
        "\n",
        "    train_cols = df.columns\n",
        "    train_cols = train_cols.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "\n",
        "    class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "\n",
        "    for fold in range(5):\n",
        "        # lr = RandomForestClassifier(max_features = None, bootstrap = False, class_weight = 'balanced_subsample',\n",
        "        # n_estimators = 1, max_depth = 2, min_impurity_decrease = 0.017, random_state = 123)\n",
        "        # lr = RandomForestClassifier(random_state = 123)\n",
        "        # lr = SVC(probability = True, kernel='rbf', random_state = 123) #, gamma='scale' C = 0.1, degree = 8,kernel = 'rbf' / 'poly', gamma = 'auto',\n",
        "        # lr  = catboost.CatBoostClassifier(verbose=0,  boosting_type='Ordered', learning_rate=0.03,\n",
        "        # loss_function='CrossEntropy', custom_metric=['BalancedAccuracy', 'AUC', 'F1'], od_pval=0, l2_leaf_reg=5, one_hot_max_size=10,\n",
        "        # bootstrap_type='Bayesian', random_strength=0.3)#, train_dir='../src/catboost_train'\n",
        "        paras = {'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'border_count': 32, 'custom_metric': 'AUC', 'depth': 6, 'feature_border_type': 'GreedyLogSum',\n",
        "         'fold_permutation_block': 1, 'grow_policy': 'SymmetricTree', 'iterations': 100, 'l2_leaf_reg': 1, 'leaf_estimation_backtracking': 'AnyImprovement',\n",
        "         'leaf_estimation_iterations': 1, 'leaf_estimation_method': 'Newton', 'learning_rate': 0.01, 'loss_function': 'CrossEntropy', 'nan_mode': 'Min',\n",
        "         'random_strength': 0.1, 'sampling_frequency': 'PerTreeLevel', 'subsample': 0.1, 'use_best_model': True}\n",
        "        # lr  = catboost.CatBoostClassifier(**paras)\n",
        "        lr = catboost.CatBoostClassifier()\n",
        "        # lr = lightgbm.LGBMClassifier(class_weight='balanced', learning_rate=0.4, min_child_samples=13, num_leaves=27, random_state=123)\n",
        "        # lr = GaussianProcessClassifier(kernel='RBF', optimizer= 'fmin_l_bfgs_b', n_restarts_optimizer= 1)\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        # sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "\n",
        "        lr.fit(train_X, train_Y)\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "\n",
        "    print(train_df)\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "    # lst.to_csv('../data/tri3_predict_proba.csv', index_label=None, index=None)\n",
        "    # return np.mean(oof_roc_auc)\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRwh83asNKOZ"
      },
      "source": [
        "## random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk17i-qQNKOa"
      },
      "outputs": [],
      "source": [
        "\n",
        "## for tri2\n",
        "lr = RandomForestClassifier(n_estimators = 56,  max_depth = 3, criterion = 'entropy', min_samples_split = 0.27, min_samples_leaf = 8,\n",
        "                                    bootstrap = False, ccp_alpha = 0.05, max_features = None, min_impurity_decrease = 0.0332, random_state = 123)\n",
        "\n",
        "\n",
        "## for tri3\n",
        "lr = RandomForestClassifier(max_features = None, bootstrap = False, class_weight = 'balanced_subsample',\n",
        "                n_estimators = 1, max_depth = 2, min_impurity_decrease = 0.017, random_state = 123)  # criterion = 'log_loss', balanced_subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B2l2WnMNKOa"
      },
      "outputs": [],
      "source": [
        "def rf(df, ctrl, label = 'sga'):\n",
        "\n",
        "    train_cols = df.columns\n",
        "    train_cols = train_cols.drop([label, 'fold'])\n",
        "\n",
        "    oof_acc = []\n",
        "    oof_roc_auc = []\n",
        "    oof_f1 = []\n",
        "    oof_prec = []\n",
        "    oof_rec = []\n",
        "    oof_thresh = []\n",
        "\n",
        "    class_weights = calc_class_weights(df, target= label, type='normal')\n",
        "\n",
        "    for fold in range(5):\n",
        "\n",
        "        ## zw sga + lbw better\n",
        "        ## this is status_change↓\n",
        "        # - tri2 a_acc=0.7771, auc=0.7753 **\n",
        "        # - tri3 a_acc=0.7775, auc=0.4329\n",
        "        lr = RandomForestClassifier(n_estimators = 56,  max_depth = 3, criterion = 'entropy', min_samples_split = 0.27, min_samples_leaf = 8,\n",
        "                                    bootstrap = False, ccp_alpha = 0.05, max_features = None, min_impurity_decrease = 0.0332, random_state = 123)\n",
        "                            # lr = RandomForestClassifier(n_estimators = 56, criterion = 'entropy', max_depth = 3, min_samples_split = 0.27, min_samples_leaf = 8,\n",
        "                                                        #   bootstrap = False, ccp_alpha = 0.05, max_features = None, min_impurity_decrease = 0.0332, random_state = 123)\n",
        "                            # lr = RandomForestClassifier(max_features = None, bootstrap = False, n_estimators = 1, max_depth = 1, criterion = \"entropy\",\n",
        "                            #                            min_impurity_decrease = 0.1, verbose = 0, random_state = 123, min_samples_leaf = 1, ccp_alpha = 0.2)\n",
        "\n",
        "        ## yifong status_change↓ better\n",
        "        # - tri2 a_acc=0.7735, auc=0.7673\n",
        "        # - tri3 a_acc=0.7775, auc=0.7024  **\n",
        "        # lr = RandomForestClassifier(max_features = None, bootstrap = False, class_weight = 'balanced_subsample',\n",
        "                    #   n_estimators = 1, max_depth = 2, min_impurity_decrease = 0.017, random_state = 123)  # criterion = 'log_loss', balanced_subsample\n",
        "\n",
        "\n",
        "        ros = RandomOverSampler(random_state = 123)\n",
        "        train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "        train_X, train_Y = ros.fit_resample(train_df[train_cols], train_df[label])\n",
        "        val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "        # sample_weights = [class_weights[int(x)] for x in train_Y]\n",
        "\n",
        "        lr.fit(train_X, train_Y)\n",
        "        out = lr.predict_proba(val_df[train_cols])\n",
        "        acc, roc_auc, f1, prec, rec = calc_metrics(val_df[label], out, metrics=[balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score])\n",
        "        out2 = lr.predict(val_df[train_cols])\n",
        "        threshold = []\n",
        "        accuracy = []\n",
        "        for p in np.unique(lr.predict_proba(train_X)[:,1]):\n",
        "            threshold.append(p)\n",
        "            y_pred = (lr.predict_proba(train_X)[:,1] >= p).astype(int)\n",
        "            accuracy.append(balanced_accuracy_score(train_Y, y_pred))\n",
        "        thresh = threshold[np.argmax(accuracy)]\n",
        "\n",
        "\n",
        "        oof_acc.append(acc)\n",
        "        oof_roc_auc.append(roc_auc)\n",
        "        oof_f1.append(f1)\n",
        "        oof_prec.append(prec)\n",
        "        oof_rec.append(rec)\n",
        "        oof_thresh.append(thresh)\n",
        "        val_df = pd.concat([val_df, pd.DataFrame(out2)], axis = 1)\n",
        "\n",
        "    # print(train_df)\n",
        "    print(lr, lr.get_params())\n",
        "    print(f'OOF Prec Score: {np.mean(oof_prec):.4f} (±{np.std(oof_prec):.4f})')\n",
        "    print(f'OOF Recall Score: {np.mean(oof_rec):.4f} (±{np.std(oof_rec):.4f})')\n",
        "    print(f'OOF F1 Score: {np.mean(oof_f1):.4f} (±{np.std(oof_f1):.4f})')\n",
        "    print(f'OOF Balanced Accuracy: {np.mean(oof_acc):.4f} (±{np.std(oof_acc):.4f})')\n",
        "    print(f'OOF ROC AUC Score: {np.mean(oof_roc_auc):.4f} (±{np.std(oof_roc_auc):.4f})')\n",
        "    # lst.to_csv('../data/tri3_predict_proba.csv', index_label=None, index=None)\n",
        "    # return np.mean(oof_roc_auc)\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJHl1AkxNKOb"
      },
      "source": [
        "# 3.Start run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt3m7wBcNKOb"
      },
      "source": [
        "## 3.1 tri2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdmz21CdNKOb",
        "outputId": "2e5ac6a9-cde3-46b6-942c-4f2b3bdae48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=False, ccp_alpha=0.05, criterion='entropy',\n",
            "                       max_depth=3, max_features=None,\n",
            "                       min_impurity_decrease=0.0332, min_samples_leaf=8,\n",
            "                       min_samples_split=0.27, n_estimators=56,\n",
            "                       random_state=123) {'bootstrap': False, 'ccp_alpha': 0.05, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0332, 'min_samples_leaf': 8, 'min_samples_split': 0.27, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 56, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}\n",
            "OOF Prec Score: 0.6459 (±0.2655)\n",
            "OOF Recall Score: 0.6194 (±0.1583)\n",
            "OOF F1 Score: 0.6179 (±0.2097)\n",
            "OOF Balanced Accuracy: 0.7771 (±0.0999)\n",
            "OOF ROC AUC Score: 0.7753 (±0.1044)\n"
          ]
        }
      ],
      "source": [
        "tri2 = pd.read_csv('./cyf_clean_tri2.csv')\n",
        "tri2.drop(['lbw', 'sga', 'cur_sga'], axis = 1, inplace = True)\n",
        "\n",
        "# tri2 = ToNormal(tri2)\n",
        "tri2 = ohe(tri2)\n",
        "tri2 = data_impute(tri2)\n",
        "\n",
        "# tri2 = remove_multicollinearity(tri2, thresh = 90)\n",
        "# tri2 = ignore_low_variance(tri2, thresh = 0.01, label = 'status_change')\n",
        "# tri2 = pca_bmi(tri2, 11)  # 11 13\n",
        "\n",
        "tri2 = sgkf(tri2, label = 'status_change')\n",
        "# mlp(tri2)\n",
        "tri2['af'] = tri2['af'].round()\n",
        "temp = rf(tri2, 0.025, label = 'status_change')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zivid_py37",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "33e29ec1394237d94d726267a88dab0a52d21005d23b116de48568181f19d7bc"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JoojAd4FNKOB",
        "bNIB3Wx3NKOK",
        "LgIcAWqjNKOU",
        "v4F4Apl9NKOV",
        "76mv89zENKOW",
        "bZvp0AotNKOW",
        "IPhMAwDFNKOX",
        "38--jc7fNKOY",
        "XwiJZpMyNKOY",
        "ErWKlJRhNKOe"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}